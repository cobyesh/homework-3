---
title: "Homework 3"
author: "Coby Eshaghian"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Classification

For this assignment, we will be working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

![Fig. 1: RMS Titanic departing Southampton on April 10, 1912.](images/RMS_Titanic.jpg){width="363"}

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

```{r}
library(tidyverse)
library(tidymodels)
library(ggplot2)
titanic <- read.csv('/Users/cobyeshaghian/Downloads/pstat 131/homework-3/data/titanic.csv')
head(titanic)
```


Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

```{r}
titanic$surv <- ifelse(titanic$survived == 'Yes', 1, 0)

head(titanic)
```


Make sure you load the `tidyverse` and `tidymodels`!

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*


### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. Take a look at the training data and note any potential issues, such as missing data.

Why is it a good idea to use stratified sampling for this data?

So that we can observe different subgroups all while they share the same statistic.

```{r}

set.seed(1999)

titanic_split <- initial_split(titanic, prop = 0.75,
                                strata = surv)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

print(titanic_train)
```


### Question 2

Using the **training** data set, explore/describe the distribution of the outcome variable `survived`.

We see that it reorginizes the data such that survived so we can compare different outcomes between those who survived and who didn't. 


### Question 3

Using the **training** data set, create a correlation matrix of all continuous variables. Create a visualization of the matrix, and describe any patterns you see. Are any predictors correlated with each other? Which ones, and in which direction?

```{r}
library(corrplot)
numtitanic_train <- titanic_train[,sapply(titanic_train,is.numeric)] #remove non-numeric numbers
numtitanic_train$passenger_id <- NULL
corrplot(cor(numtitanic_train), method = 'number', type = 'lower') #normalize the data within correlation parameters [-1,1]
```


### Question 4

Using the **training** data, create a recipe predicting the outcome variable `survived`. Include the following predictors: ticket class, sex, age, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.

Recall that there were missing values for `age`. To deal with this, add an imputation step using `step_impute_linear()`. Next, use `step_dummy()` to **dummy** encode categorical predictors. Finally, include interactions between:

-   Sex and passenger fare, and
-   Age and passenger fare.

```{r}
titrec <- recipe(survived ~  pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_center(all_numeric_predictors()) %>%
  step_scale((all_numeric_predictors())) %>%
  step_impute_linear(age)

intmod1 <- titrec %>%
  step_interact(terms = ~ sex:fare)

intmod1 <- prep(intmod1, training = titanic_train)

intmod2 <- titrec %>%
  step_interact(terms = ~ age:fare)

intmod2 <- prep(intmod2, training = titanic_train)
```


You'll need to investigate the `tidymodels` documentation to find the appropriate step functions to use.

### Question 5

Specify a **logistic regression** model for classification using the `"glm"` engine. Then create a workflow. Add your model and the appropriate recipe. Finally, use `fit()` to apply your workflow to the **training** data.

***Hint: Make sure to store the results of `fit()`. You'll need them later on.***

```{r}
log_model <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm")

log_wflow <- workflow() %>% 
  add_model(log_model) %>% 
  add_recipe(titrec)

log_fit <- fit(log_wflow, titanic_train)
```

### Question 6

**Repeat Question 5**, but this time specify a linear discriminant analysis model for classification using the `"MASS"` engine.

```{r}
library(MASS)
library(discrim)


lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titrec)

lda_fit <- fit(lda_wkflow, titanic_train)
```


### Question 7

**Repeat Question 5**, but this time specify a quadratic discriminant analysis model for classification using the `"MASS"` engine.

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titrec)

qda_fit <- fit(qda_wkflow, titanic_train)
```


### Question 8

**Repeat Question 5**, but this time specify a naive Bayes model for classification using the `"klaR"` engine. Set the `usekernel` argument to `FALSE`.

```{r}

library(caret)
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titrec)

nb_fit <- fit(nb_wkflow, titanic_train)
```


### Question 9

Now you've fit four different models to your training data.

Use `predict()` and `bind_cols()` to generate predictions using each of these 4 models and your **training** data. Then use the *accuracy* metric to assess the performance of each of the four models.

```{r}
logpre <- predict(log_fit, new_data = titanic_train, type = "prob")

ldapre <- predict(lda_fit, new_data = titanic_train, type = "prob")

qdapre <- predict(qda_fit, new_data = titanic_train, type = "prob")

nbpre <- predict(nb_fit, new_data = titanic_train, type = "prob")

library("dplyr") 

log_reg_acc <- augment(log_fit, new_data = titanic_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)

lda_acc <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)

qda_acc <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)

nb_acc <- augment(nb_fit, new_data = titanic_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)

accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```


Which model achieved the highest accuracy on the training data?

**The logistic regression model had the highest accuracy on the training data.** 

### Question 10

Fit the model with the highest training accuracy to the **testing** data. Report the accuracy of the model on the **testing** data.

Again using the **testing** data, create a confusion matrix and visualize it. Plot an ROC curve and calculate the area under it (AUC).

```{r}
lgtest_fit <- fit(log_wflow, titanic_test)

lgtest_acc <- augment(lgtest_fit, new_data = titanic_test) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
lgtest_acc

augment(lgtest_fit, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

#augment(lgtest_fit, new_data = titanic_test) %>%
  #roc_curve(truth = survived) %>%
  #autoplot()

#for some reason, code above isn't working. I reached out to hanmo to get resolved but wasn't able to find a time that worked for both of us...
```


How did the model perform? Compare its training and testing accuracies. If the values differ, why do you think this is so?

Model performed well. However, it had a lower testing accuracy than training, given the training worked with a different/smaller subset of the greater data set. Hence, we may have experienced some overfitting...